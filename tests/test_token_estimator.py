"""
æµ‹è¯•Tokenä¼°ç®—æœåŠ¡
"""

from pathlib import Path

import pytest

from services.token_estimator import estimate_tokens


class TestEstimateTokens:
    """æµ‹è¯•estimate_tokenså‡½æ•°"""

    def test_estimate_tokens_empty_file(self, tmp_path):
        """æµ‹è¯•ç©ºæ–‡ä»¶çš„tokenä¼°ç®—"""
        empty_file = tmp_path / "empty.txt"
        empty_file.write_text("", encoding="utf-8")

        # ç©ºæ–‡ä»¶ä¼šæŠ›å‡ºProcessingError
        from exceptions import ProcessingError
        with pytest.raises(ProcessingError, match="æ–‡æœ¬å†…å®¹ä¸ºç©º"):
            estimate_tokens(str(empty_file))

    def test_estimate_tokens_small_file(self, tmp_path):
        """æµ‹è¯•å°æ–‡ä»¶çš„tokenä¼°ç®—"""
        small_file = tmp_path / "small.txt"
        small_file.write_text("Hello world. This is a test.", encoding="utf-8")

        result = estimate_tokens(str(small_file))

        assert result["total_tokens"] > 0
        assert result["chunk_tokens"] > 0
        assert result["chunk_responses"] >= 0
        assert result["merge_tokens"] >= 0
        assert result["total_estimated"] > 0
        assert result["chunk_count"] >= 1

    def test_estimate_tokens_large_file(self, tmp_path):
        """æµ‹è¯•å¤§æ–‡ä»¶çš„tokenä¼°ç®—"""
        large_file = tmp_path / "large.txt"
        large_file.write_text("Hello world. " * 1000, encoding="utf-8")

        result = estimate_tokens(str(large_file))

        assert result["total_tokens"] > 0
        assert result["chunk_tokens"] > 0
        assert result["chunk_responses"] > 0
        assert result["merge_tokens"] > 0
        assert result["total_estimated"] > 0
        assert result["chunk_count"] >= 1

    def test_estimate_tokens_with_chinese(self, tmp_path):
        """æµ‹è¯•åŒ…å«ä¸­æ–‡çš„æ–‡ä»¶tokenä¼°ç®—"""
        chinese_file = tmp_path / "chinese.txt"
        chinese_file.write_text("è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡ä»¶ã€‚åŒ…å«ä¸€äº›ä¸­æ–‡å†…å®¹ã€‚", encoding="utf-8")

        result = estimate_tokens(str(chinese_file))

        assert result["total_tokens"] > 0
        assert result["chunk_tokens"] > 0
        assert result["chunk_responses"] >= 0
        assert result["merge_tokens"] >= 0
        assert result["total_estimated"] > 0
        assert result["chunk_count"] >= 1

    def test_estimate_tokens_with_newlines(self, tmp_path):
        """æµ‹è¯•åŒ…å«æ¢è¡Œç¬¦çš„æ–‡ä»¶tokenä¼°ç®—"""
        newline_file = tmp_path / "newline.txt"
        content = "Line 1\nLine 2\nLine 3\nLine 4\nLine 5"
        newline_file.write_text(content, encoding="utf-8")

        result = estimate_tokens(str(newline_file))

        assert result["total_tokens"] > 0
        assert result["chunk_tokens"] > 0
        assert result["chunk_responses"] >= 0
        assert result["merge_tokens"] >= 0
        assert result["total_estimated"] > 0
        assert result["chunk_count"] >= 1

    def test_estimate_tokens_chunk_responses_calculation(self, tmp_path):
        """æµ‹è¯•chunk_responsesè®¡ç®—ï¼ˆåº”ä¸ºchunk_tokensçš„30%ï¼‰"""
        test_file = tmp_path / "test.txt"
        test_file.write_text("Hello world. " * 100, encoding="utf-8")

        result = estimate_tokens(str(test_file))

        assert result["chunk_responses"] == int(result["chunk_tokens"] * 0.3)

    def test_estimate_tokens_merge_tokens_calculation(self, tmp_path):
        """æµ‹è¯•merge_tokensè®¡ç®—ï¼ˆåº”ä¸ºtotal_tokensçš„10%ï¼‰"""
        test_file = tmp_path / "test.txt"
        test_file.write_text("Hello world. " * 100, encoding="utf-8")

        result = estimate_tokens(str(test_file))

        assert result["merge_tokens"] == int(result["total_tokens"] * 0.1)

    def test_estimate_tokens_total_estimated_calculation(self, tmp_path):
        """æµ‹è¯•total_estimatedè®¡ç®—"""
        test_file = tmp_path / "test.txt"
        test_file.write_text("Hello world. " * 100, encoding="utf-8")

        result = estimate_tokens(str(test_file))

        expected_total = (
            result["chunk_tokens"] +
            result["chunk_responses"] +
            result["merge_tokens"]
        )
        assert result["total_estimated"] == expected_total

    def test_estimate_tokens_chunk_count(self, tmp_path):
        """æµ‹è¯•chunk_countæ­£ç¡®æ€§"""
        test_file = tmp_path / "test.txt"
        test_file.write_text("Hello world. " * 100, encoding="utf-8")

        result = estimate_tokens(str(test_file))

        assert result["chunk_count"] >= 1
        assert isinstance(result["chunk_count"], int)

    def test_estimate_tokens_file_not_exists(self):
        """æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨çš„æƒ…å†µ"""
        with pytest.raises(FileNotFoundError):
            estimate_tokens("nonexistent_file.txt")

    def test_estimate_tokens_return_structure(self, tmp_path):
        """æµ‹è¯•è¿”å›žå€¼ç»“æž„"""
        test_file = tmp_path / "test.txt"
        test_file.write_text("Hello world.", encoding="utf-8")

        result = estimate_tokens(str(test_file))

        assert isinstance(result, dict)
        assert "total_tokens" in result
        assert "chunk_tokens" in result
        assert "chunk_responses" in result
        assert "merge_tokens" in result
        assert "total_estimated" in result
        assert "chunk_count" in result

        # æ£€æŸ¥æ‰€æœ‰å€¼éƒ½æ˜¯æ•´æ•°
        for key, value in result.items():
            assert isinstance(value, int), f"{key} should be int, got {type(value)}"

    def test_estimate_tokens_with_special_characters(self, tmp_path):
        """æµ‹è¯•åŒ…å«ç‰¹æ®Šå­—ç¬¦çš„æ–‡ä»¶tokenä¼°ç®—"""
        special_file = tmp_path / "special.txt"
        special_file.write_text("Hello! @#$%^&*() World! 12345", encoding="utf-8")

        result = estimate_tokens(str(special_file))

        assert result["total_tokens"] > 0
        assert result["chunk_tokens"] > 0
        assert result["total_estimated"] > 0

    def test_estimate_tokens_with_emoji(self, tmp_path):
        """æµ‹è¯•åŒ…å«emojiçš„æ–‡ä»¶tokenä¼°ç®—"""
        emoji_file = tmp_path / "emoji.txt"
        emoji_file.write_text("Hello ðŸ˜Š World ðŸŽ‰", encoding="utf-8")

        result = estimate_tokens(str(emoji_file))

        assert result["total_tokens"] > 0
        assert result["chunk_tokens"] > 0
        assert result["total_estimated"] > 0

    def test_estimate_tokens_single_chunk(self, tmp_path):
        """æµ‹è¯•åªæœ‰ä¸€ä¸ªå—çš„æƒ…å†µ"""
        single_chunk_file = tmp_path / "single.txt"
        single_chunk_file.write_text("Short text.", encoding="utf-8")

        result = estimate_tokens(str(single_chunk_file))

        assert result["chunk_count"] == 1

    def test_estimate_tokens_multiple_chunks(self, tmp_path):
        """æµ‹è¯•å¤šä¸ªå—çš„æƒ…å†µ"""
        multi_chunk_file = tmp_path / "multi.txt"
        # åˆ›å»ºè¶³å¤Ÿé•¿çš„æ–‡æœ¬ä»¥äº§ç”Ÿå¤šä¸ªå—
        multi_chunk_file.write_text("Hello world. " * 1000, encoding="utf-8")

        result = estimate_tokens(str(multi_chunk_file))

        assert result["chunk_count"] >= 1

    def test_estimate_tokens_consistency(self, tmp_path):
        """æµ‹è¯•å¤šæ¬¡è°ƒç”¨ç»“æžœä¸€è‡´"""
        test_file = tmp_path / "test.txt"
        test_file.write_text("Hello world. " * 100, encoding="utf-8")

        result1 = estimate_tokens(str(test_file))
        result2 = estimate_tokens(str(test_file))

        assert result1 == result2